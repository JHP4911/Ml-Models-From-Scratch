{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename'])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "\n",
    "data = datasets.load_iris()\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = data['data'],data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9933333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier,ExtraTreesRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=10)\n",
    "model.fit(X,Y)\n",
    "print(accuracy_score(Y,model.predict(X)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExtraTreesRegressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Random Forest Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_subsets(X, y, n_subsets, replacements=True):\n",
    "    \n",
    "    n_samples = np.shape(X)[0]\n",
    "    # Concatenate x and y and do a random shuffle\n",
    "    X_y = np.concatenate((X, np.expand_dims(y,axis = 1)), axis=1)\n",
    "    np.random.shuffle(X_y)\n",
    "    subsets = []\n",
    "\n",
    "    # Uses 50% of training samples without replacements\n",
    "    subsample_size = int(n_samples // 2)\n",
    "    if replacements:\n",
    "        subsample_size = n_samples      # 100% with replacements\n",
    "\n",
    "    for _ in range(n_subsets):\n",
    "        idx = np.random.choice(\n",
    "            range(n_samples),\n",
    "            size=subsample_size,\n",
    "            replace=replacements)\n",
    "        \n",
    "        X = X_y[idx][:, :-1]\n",
    "        y = X_y[idx][:, -1]\n",
    "        \n",
    "        subsets.append([X, y])\n",
    "    return subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest:\n",
    "    \n",
    "    def __init__(self,n_estimators = 3,min_samples_split=3, min_impurity=1e-7,\n",
    "                 max_depth= 5,criterian = 'entropy',max_features = 'sqrt'):\n",
    "        \n",
    "        self.n_estimators = n_estimators\n",
    "        self.tree = []\n",
    "        self.feature_idx = []\n",
    "        self.max_features = max_features\n",
    "        \n",
    "        for i in range(self.n_estimators):\n",
    "        \n",
    "            self.tree.append(DecisionTreeClassifier(min_samples_split,\n",
    "                                                     min_impurity,\n",
    "                                                     max_depth,criterian,\n",
    "                                                    ))\n",
    "        \n",
    "        \n",
    "    def fit(self,X,Y):    \n",
    "        \n",
    "        n_samples,n_features = X.shape\n",
    "        \n",
    "        \n",
    "        #     - If \"auto\", then `max_features=sqrt(n_features)`.\n",
    "        #     - If \"sqrt\", then `max_features=sqrt(n_features)` (same as \"auto\").\n",
    "        #     - If \"log2\", then `max_features=log2(n_features)`.\n",
    "        \n",
    "        #number of features to consider \n",
    "        \n",
    "        if self.max_features == None:\n",
    "            self.max_features = n_features\n",
    "        elif self.max_features == 'sqrt':\n",
    "            self.max_features = np.sqrt(n_features).astype('int32')\n",
    "        else:\n",
    "            self.max_features = np.log2(n_features).astype('int32')\n",
    "            \n",
    "    \n",
    "        \n",
    "        #random forest uses bagging in which we \n",
    "        #1. need to create sub-samples of the dataset : bootstraping \n",
    "        #2. then train each tree on a unique tree and aggregate the results \n",
    "        \n",
    "        subsets  = get_random_subsets(X,Y,self.n_estimators)\n",
    "            \n",
    "#         print(self.max_features,type(self.max_features))    \n",
    "        \n",
    "        for i in range(self.n_estimators):\n",
    "            \n",
    "            #get the sampled dataset \n",
    "            X_,Y_  = subsets[i]\n",
    "            \n",
    "            #choose random features \n",
    "            \n",
    "            idx = np.random.choice(range(n_features), size=self.max_features, replace=False)\n",
    "            # Save the indices of the features for prediction\n",
    "            \n",
    "            self.feature_idx.append(idx)\n",
    "            \n",
    "            # Choose the features corresponding to the indices\n",
    "            X_ = X_[:, idx]\n",
    "          \n",
    "            self.tree[i].fit(X_,Y_)\n",
    "        \n",
    "        \n",
    "    def predict(self,X):\n",
    "        \n",
    "        preds = np.zeros((X.shape[0],self.n_estimators))\n",
    "        for i in range(self.n_estimators):\n",
    "            \n",
    "            #select the proper indices \n",
    "            X_ = X[:,self.feature_idx[i]]\n",
    "            \n",
    "            preds[:,i] = self.tree[i].predict(X_)\n",
    "        \n",
    "        final_pred = []\n",
    "        for pred in preds:\n",
    "            \n",
    "            final_pred.append(majority_vote(pred))\n",
    "        \n",
    "        \n",
    "#         return np.round(np.mean(preds,1))\n",
    "\n",
    "        return final_pred\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9866666666666667"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RandomForest(n_estimators= 10,criterian='gini')\n",
    "model.fit(X,Y)\n",
    "# model.predict(X)\n",
    "accuracy_score(Y,model.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " np.random.choice(range(4), size=2, replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionNode:\n",
    "    \n",
    "    def __init__(self,feature = None,\n",
    "                     entropy = None,threshold = None,\n",
    "                     value = None,right=None,left=None):   \n",
    "            \n",
    "            self.entropy = entropy\n",
    "            \n",
    "            self.feature = feature\n",
    "             \n",
    "            self.right = right\n",
    "            \n",
    "            self.left = left\n",
    "            \n",
    "            self.threshold = threshold\n",
    "            \n",
    "            self.value = value\n",
    "\n",
    "# function to find feature to split on \n",
    "\n",
    "def divide_on_feature(X, feature_i, threshold):\n",
    "\n",
    "    split_func = None\n",
    "    if isinstance(threshold, int) or isinstance(threshold, float):\n",
    "        split_func = lambda sample: sample[feature_i] >= threshold\n",
    "    else:\n",
    "        split_func = lambda sample: sample[feature_i] == threshold\n",
    "\n",
    "    X_1 = np.array([sample for sample in X if split_func(sample)])\n",
    "    X_2 = np.array([sample for sample in X if not split_func(sample)])\n",
    "\n",
    "    return np.array([X_1, X_2])\n",
    "\n",
    "\n",
    "def entropy(y):\n",
    "    \n",
    "    classes = np.unique(y) # 1 2 3\n",
    "    length = len(y)\n",
    "    entropy = 0\n",
    "    for label in classes:\n",
    "        p = np.sum(y == label) / length\n",
    "        entropy += p * np.log2(p)\n",
    "        \n",
    "    return -entropy\n",
    "\n",
    "\n",
    "def gini_index(y):\n",
    "    \n",
    "    # formula : 1 - p+ ^ 2 - p-^2 \n",
    "    \n",
    "    classes = np.unique(y) # 1 2 3\n",
    "    length = len(y)\n",
    "    gini = 1\n",
    "    proba = 0\n",
    "    for label in classes:\n",
    "        p = np.sum(y == label) / length\n",
    "        proba += p **2\n",
    "        \n",
    "    return gini - proba\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def majority_vote(y):\n",
    "    \n",
    "        most_common = None\n",
    "        max_count = 0\n",
    "        for label in np.unique(y):\n",
    "            # Count number of occurences of samples with label\n",
    "            count = len(y[y == label])\n",
    "            if count > max_count:\n",
    "                most_common = label\n",
    "                max_count = count\n",
    "        return most_common\n",
    "\n",
    "    \n",
    "\n",
    "class DecisionTreeClassifier:\n",
    "    \n",
    "    \n",
    "    def __init__(self, min_samples_split=3, min_impurity=1e-7,\n",
    "                 max_depth= 5,criterian = 'entropy'):\n",
    "        \n",
    "        #contains the root of the tree \n",
    "        self.root = None         \n",
    "        self.criterian = criterian\n",
    "        #stopping conditions \n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_impurity = min_impurity\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        \n",
    "        self.one_dim = len(np.shape(y)) == 1\n",
    "\n",
    "        \n",
    "#         print(f'Shape of dataset :: {X.shape}')\n",
    "        \n",
    "        self.root  = self._build_tree(X,y)\n",
    "        \n",
    "    \n",
    "        \n",
    "    def _build_tree(self, X, y, current_depth=0):\n",
    "       \n",
    "\n",
    "        largest_impurity = 0\n",
    "        best_criteria = None    # Feature index and threshold\n",
    "        best_sets = None     # Subsets of the data\n",
    "        right = None \n",
    "        left= None\n",
    "        selected_feature = None\n",
    "        selected_threshold = None\n",
    "        \n",
    "        # Check if expansion of y is needed\n",
    "        if len(np.shape(y)) == 1:\n",
    "            y = np.expand_dims(y, axis=1)\n",
    "\n",
    "        # Add y as last column of X\n",
    "        Xy = np.concatenate((X, y), axis=1)\n",
    "\n",
    "        n_samples, n_features = np.shape(X)\n",
    "\n",
    "        \n",
    "        if n_samples >= self.min_samples_split and self.max_depth >= current_depth:\n",
    "            \n",
    "#             print(current_depth)\n",
    "            \n",
    "            # Calculate the impurity for each feature\n",
    "            for feature_i in range(n_features):\n",
    "                \n",
    "                # All values of feature_i\n",
    "                feature_values = np.expand_dims(X[:, feature_i], axis=1)\n",
    "                unique_values = np.unique(feature_values)\n",
    "\n",
    "                # Iterate through all unique values of feature column i and\n",
    "                # calculate the impurity\n",
    "                for threshold in unique_values:\n",
    "                    # Divide X and y depending on if the feature value of X at index feature_i\n",
    "                    # meets the threshold\n",
    "                    Xy1, Xy2 = divide_on_feature(Xy, feature_i, threshold)\n",
    "\n",
    "                    if len(Xy1) > 0 and len(Xy2) > 0:\n",
    "                        # Select the y-values of the two sets\n",
    "                        y1 = Xy1[:,-1]\n",
    "                        y2 = Xy2[:,-1]\n",
    "\n",
    "                        # Calculate impurity\n",
    "                        impurity = self.info_gain(y, y1, y2)\n",
    "\n",
    "                        # If this threshold resulted in a higher information gain than previously\n",
    "                        # recorded save the threshold value and the feature\n",
    "                        # index\n",
    "                        if impurity > largest_impurity:\n",
    "                            largest_impurity = impurity\n",
    "                            selected_feature = feature_i\n",
    "                            selected_threshold = threshold \n",
    "                            right,left = Xy2,Xy1\n",
    "    \n",
    "    \n",
    "        if largest_impurity > self.min_impurity:\n",
    "            # Build subtrees for the right and left branches\n",
    "            left = self._build_tree(left[:,:-1], left[:,-1], current_depth = current_depth + 1)\n",
    "            right = self._build_tree(right[:,:-1], right[:,-1], current_depth = current_depth + 1)\n",
    "            return DecisionNode(feature=selected_feature, threshold=selected_threshold, left=left, right=right)\n",
    "\n",
    "        # We're at leaf => determine value\n",
    "        leaf_value = majority_vote(y)\n",
    "\n",
    "        return DecisionNode(value=leaf_value)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_imputiry_metric(self):\n",
    "        \n",
    "        if self.criterian == 'entropy':\n",
    "            return entropy\n",
    "        else:\n",
    "            return gini_index\n",
    "            \n",
    "    \n",
    "    def info_gain(self,y,y1,y2):\n",
    "    \n",
    "        p = len(y1) / len(y)\n",
    "              \n",
    "        criterian = self.get_imputiry_metric()\n",
    "\n",
    "        ent = criterian(y)\n",
    "\n",
    "        info_gain = ent - p * criterian(y1) - (1 - p) * criterian(y2)\n",
    "\n",
    "        return info_gain\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    def predict_value(self, x, tree=None):\n",
    "        \n",
    "\n",
    "        if tree is None:\n",
    "            tree = self.root\n",
    "\n",
    "        if tree.value is not None:\n",
    "            return tree.value\n",
    "\n",
    "        feature_value = x[tree.feature]\n",
    "\n",
    "        # Determine if we will follow left or right branch\n",
    "        branch = tree.right\n",
    "        if isinstance(feature_value, int) or isinstance(feature_value, float):\n",
    "            if feature_value >= tree.threshold:\n",
    "                branch = tree.left\n",
    "        elif feature_value == tree.threshold:\n",
    "            branch = tree.left\n",
    "\n",
    "\n",
    "        # Test subtree\n",
    "        return self.predict_value(x, branch)\n",
    "\n",
    "        \n",
    "    def predict(self,X):\n",
    "        \n",
    "        y_pred = [self.predict_value(sample) for sample in X]\n",
    "         \n",
    "        return y_pred\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForest Vs ExtraTreeClfs\n",
    "\n",
    "![](Q18mk.png)\n",
    "\n",
    "ExtraTreesClassifier is like a brother of RandomForest but with 2 important differences.\n",
    "\n",
    "We are building multiple decision trees. For building multiple trees, we need multiple datasets. Best practice is that we don't train the decision trees on the complete dataset but we train only on fraction of data (around 80%) for each tree. In a random forest, we draw observations with replacement. So we can have repetition of observations in a random forest. In an ExtraTreesClassifier, we are drawing observations without replacement, so we will not have repetition of observations like in random forest.\n",
    "\n",
    "The split is the process of converting a non-homogeneous parent node into 2 homogeneous child node (best possible). In RandomForest, it select the best split to convert the parent into the two most homogeneous child nodes. In an ExtraTreesClassifier, it selects a random split to divide the parent node into two random child nodes.\n",
    "\n",
    "Let’s look at some ensemble methods ordered from high to low variance, ending in ExtraTreesClassifier.\n",
    "\n",
    "\n",
    "\n",
    "ExtraTreesClassifier is like a brother of RandomForest but with 2 important differences.\n",
    "\n",
    "enter image description here\n",
    "\n",
    "We are building multiple decision trees. For building multiple trees, we need multiple datasets. Best practice is that we don't train the decision trees on the complete dataset but we train only on fraction of data (around 80%) for each tree. In a random forest, we draw observations with replacement. So we can have repetition of observations in a random forest. In an ExtraTreesClassifier, we are drawing observations without replacement, so we will not have repetition of observations like in random forest.\n",
    "\n",
    "The split is the process of converting a non-homogeneous parent node into 2 homogeneous child node (best possible). In RandomForest, it select the best split to convert the parent into the two most homogeneous child nodes. In an ExtraTreesClassifier, it selects a random split to divide the parent node into two random child nodes.\n",
    "\n",
    "Let’s look at some ensemble methods ordered from high to low variance, ending in ExtraTreesClassifier.\n",
    "\n",
    "1. Decision Tree (High Variance)\n",
    "\n",
    "A single decision tree is usually overfits the data it is learning from because it learn from only one pathway of decisions. Predictions from a single decision tree usually don’t make accurate predictions on new data.\n",
    "\n",
    "2. Random Forest (Medium Variance)\n",
    "\n",
    "Random forest models reduce the risk of overfitting by introducing randomness by:\n",
    "\n",
    "    building multiple trees (n_estimators)\n",
    "    drawing observations with replacement (i.e., a bootstrapped sample)\n",
    "    splitting nodes on the best split among a random subset of the features selected at every node. Split is process to convert non-homogeneous parent node into 2 homogeneous child node(best possible).\n",
    "\n",
    "3. Extra Trees (Low Variance)\n",
    "\n",
    "Extra Trees is like a Random Forest, in that it builds multiple trees and splits nodes using random subsets of features, but with two key differences: it does not bootstrap observations (meaning it samples without replacement), and nodes are split on random splits, not best splits. So in summary, ExtraTrees:\n",
    "\n",
    "    builds multiple trees with bootstrap = False by default, which means it samples without replacement\n",
    "    nodes are split based on random splits among a random subset of the features selected at every node\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# to make RandomForest Act like decision trees \n",
    "1. set replacement = False\n",
    "2. split on a random sampple from a fraction of features "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
