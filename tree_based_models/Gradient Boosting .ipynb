{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename'])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "\n",
    "data = datasets.load_iris()\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = data['data'],data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9933333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = GradientBoostingClassifier(n_estimators=10)\n",
    "model.fit(X,Y)\n",
    "print(accuracy_score(Y,model.predict(X)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Regressor Modified to handle One-hot-encoded labels\n",
    "\n",
    "\n",
    "note: for multi-class problems the labels need to be convered to one-hot encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionNode:\n",
    "    \n",
    "    def __init__(self,feature = None,\n",
    "                     entropy = None,threshold = None,\n",
    "                     value = None,right=None,left=None):   \n",
    "            \n",
    "            self.entropy = entropy\n",
    "            \n",
    "            self.feature = feature\n",
    "             \n",
    "            self.right = right\n",
    "            \n",
    "            self.left = left\n",
    "            \n",
    "            self.threshold = threshold\n",
    "            \n",
    "            self.value = value\n",
    "\n",
    "# function to find feature to split on \n",
    "\n",
    "def divide_on_feature(X, feature_i, threshold):\n",
    "\n",
    "    split_func = None\n",
    "    if isinstance(threshold, int) or isinstance(threshold, float):\n",
    "        split_func = lambda sample: sample[feature_i] >= threshold\n",
    "    else:\n",
    "        split_func = lambda sample: sample[feature_i] == threshold\n",
    "\n",
    "    X_1 = np.array([sample for sample in X if split_func(sample)])\n",
    "    X_2 = np.array([sample for sample in X if not split_func(sample)])\n",
    "\n",
    "    return np.array([X_1, X_2])\n",
    "\n",
    "\n",
    "\n",
    "class DecisionTreeRegressior:\n",
    "    \n",
    "    \n",
    "    def __init__(self, min_samples_split=3, min_impurity=1e-7,\n",
    "                 max_depth= 5,criterian = 'mse'):\n",
    "        \n",
    "        #contains the root of the tree \n",
    "        self.root = None         \n",
    "        self.criterian = criterian\n",
    "        #stopping conditions \n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_impurity = min_impurity\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def fit(self,X,y):\n",
    "\n",
    "        self.root  = self._build_tree(X,y)\n",
    "        \n",
    "    \n",
    "        \n",
    "    def _build_tree(self, X, y, current_depth=0):\n",
    " \n",
    "        largest_impurity = 0\n",
    "        best_criteria = None    # Feature index and threshold\n",
    "        best_sets = None     # Subsets of the data\n",
    "        right = None \n",
    "        left= None\n",
    "        selected_feature = None\n",
    "        selected_threshold = None\n",
    "        \n",
    "        # Check if expansion of y is needed\n",
    "        if len(np.shape(y)) == 1:\n",
    "            y = np.expand_dims(y, axis=1)\n",
    "\n",
    "        # Add y as last column of X\n",
    "        Xy = np.concatenate((X, y), axis=1)\n",
    "\n",
    "        n_samples, n_features = np.shape(X)\n",
    "\n",
    "        \n",
    "        if n_samples >= self.min_samples_split and self.max_depth >= current_depth:\n",
    "            \n",
    "#             print(current_depth)\n",
    "            \n",
    "            # Calculate the impurity for each feature\n",
    "            for feature_i in range(n_features):\n",
    "                \n",
    "                # All values of feature_i\n",
    "                feature_values = np.expand_dims(X[:, feature_i], axis=1)\n",
    "                unique_values = np.unique(feature_values)\n",
    "\n",
    "                # Iterate through all unique values of feature column i and\n",
    "                # calculate the impurity\n",
    "                for threshold in unique_values:\n",
    "                    # Divide X and y depending on if the feature value of X at index feature_i\n",
    "                    # meets the threshold\n",
    "                    Xy1, Xy2 = divide_on_feature(Xy, feature_i, threshold)\n",
    "\n",
    "                    if len(Xy1) > 0 and len(Xy2) > 0:\n",
    "                        # Select the y-values of the two sets\n",
    "                        y1 = Xy1[:,n_features:]\n",
    "                        y2 = Xy2[:,n_features:]\n",
    "\n",
    "                        # Calculate impurity\n",
    "                        \n",
    "#                         impurity = self.variance_reduction(y, y1, y2)\n",
    "                        impurity = self.variance_reduction(y, y1, y2)\n",
    "#                      \n",
    "\n",
    "                        # If this threshold resulted in a higher information gain than previously\n",
    "                        # recorded save the threshold value and the feature\n",
    "                        # index\n",
    "                        if impurity > largest_impurity:\n",
    "                            largest_impurity = impurity\n",
    "                            selected_feature = feature_i\n",
    "                            selected_threshold = threshold \n",
    "                            right,left = Xy2,Xy1\n",
    "    \n",
    "    \n",
    "        if largest_impurity > self.min_impurity:\n",
    "            # Build subtrees for the right and left branches\n",
    "            left = self._build_tree(left[:,:n_features], left[:,n_features:], current_depth = current_depth + 1)\n",
    "            right = self._build_tree(right[:,:n_features], right[:,n_features:], current_depth = current_depth + 1)\n",
    "            return DecisionNode(feature=selected_feature, threshold=selected_threshold, left=left, right=right)\n",
    "\n",
    "        # We're at leaf => determine value\n",
    "        leaf_value = self.mean_leaf(y)\n",
    "\n",
    "        return DecisionNode(value=leaf_value)\n",
    "    \n",
    "    \n",
    "    def mean_leaf(self,y):\n",
    "        \n",
    "        value =  np.mean(y,axis = 0)\n",
    "        return value if len(value) > 1 else value[0]\n",
    "    \n",
    "    \n",
    "\n",
    "    def variance_reduction(self,y,y1,y2):\n",
    "    \n",
    "        frac1 = len(y1)/ len(y)\n",
    "        frac2 = len(y2)/ len(y)\n",
    "        return np.sum(np.var(y,axis = 0) - frac1 * np.var(y1,axis = 0) - frac2 * np.var(y2,axis=0))\n",
    "\n",
    "     \n",
    "\n",
    "    def predict_value(self, x, tree=None):\n",
    "        \n",
    "\n",
    "        if tree is None:\n",
    "            tree = self.root\n",
    "\n",
    "        if tree.value is not None:\n",
    "            return tree.value\n",
    "#         print(tree.feature)\n",
    "        feature_value = x[tree.feature]\n",
    "\n",
    "        # Determine if we will follow left or right branch\n",
    "        branch = tree.right\n",
    "        if isinstance(feature_value, int) or isinstance(feature_value, float):\n",
    "            if feature_value >= tree.threshold:\n",
    "                branch = tree.left\n",
    "        elif feature_value == tree.threshold:\n",
    "            branch = tree.left\n",
    "\n",
    "\n",
    "        # Test subtree\n",
    "        return self.predict_value(x, branch)\n",
    "\n",
    "        \n",
    "    def predict(self,X):\n",
    "        \n",
    "        y_pred = [self.predict_value(sample) for sample in X]\n",
    "         \n",
    "        return y_pred\n",
    "        \n",
    "      \n",
    "# m  = DecisionTreeRegressior()        \n",
    "# m.fit(X,Y)\n",
    "# r2_score(Y,m.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient boosting model works in a sequential manner where the next model will take in \n",
    "the previous models prediction as its ground throuth and try to reduce the error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(object):\n",
    "    def loss(self, y_true, y_pred):\n",
    "        return NotImplementedError()\n",
    "\n",
    "    def gradient(self, y, y_pred):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def acc(self, y, y_pred):\n",
    "        return 0\n",
    "\n",
    "class SquareLoss(Loss):\n",
    "    def __init__(self): pass\n",
    "\n",
    "    def loss(self, y, y_pred):\n",
    "        return 0.5 * np.power((y - y_pred), 2)\n",
    "\n",
    "    def gradient(self, y, y_pred):\n",
    "        return -(y - y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropy():\n",
    "\n",
    "\n",
    "    def loss(self, y, p):\n",
    "        # Avoid division by zero\n",
    "        p = np.clip(p, 1e-15, 1 - 1e-15)\n",
    "        return - y * np.log(p) - (1 - y) * np.log(1 - p)\n",
    "\n",
    "    def acc(self, y, p):\n",
    "        return accuracy_score(np.argmax(y, axis=1), np.argmax(p, axis=1))\n",
    "\n",
    "    def gradient(self, y, p):\n",
    "        # Avoid division by zero\n",
    "        p = np.clip(p, 1e-15, 1 - 1e-15)\n",
    "        return - (y / p) + (1 - y) / (1 - p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1,2,3])\n",
    "\n",
    "func = lambda x : np.where(a == x)[0][0]\n",
    "func(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_categorical(x):\n",
    "    \n",
    "    unique_vals = np.unique(x)\n",
    "    func = lambda x : np.where(unique_vals == x)[0][0]\n",
    "    \n",
    "    X_ = np.zeros((x.shape[0],len(unique_vals)))\n",
    "    \n",
    "    for index,val in enumerate(x):\n",
    "        \n",
    "        idx = func(val)\n",
    "        X_[index,idx] = 1\n",
    "    \n",
    "    return X_\n",
    "    \n",
    "\n",
    "class GradientBoostingClassifier:\n",
    "   \n",
    "    def __init__(self, n_estimators = 5, learning_rate = 0.1, min_samples_split=2,\n",
    "                 min_impurity = 1e-7, max_depth = 3):\n",
    "        \n",
    "        \n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_impurity = min_impurity\n",
    "        self.max_depth = max_depth\n",
    "        self.loss = CrossEntropy()\n",
    "\n",
    "        self.trees = []\n",
    "        for _ in range(n_estimators):\n",
    "            tree = DecisionTreeRegressior( min_samples_split,\n",
    "                 min_impurity, max_depth)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y = to_categorical(y)\n",
    "        y_pred = np.full(np.shape(y), np.mean(y, axis=0))\n",
    "#         print(y_pred.shape)\n",
    "        for i in range(self.n_estimators):\n",
    "        \n",
    "            gradient = self.loss.gradient(y, y_pred)\n",
    "            \n",
    "#             print(gradient.shape)\n",
    "            self.trees[i].fit(X, gradient)\n",
    "            update = self.trees[i].predict(X)\n",
    "        \n",
    "            # Update y prediction\n",
    "#             print(y_pred.shape,np.asarray(update).shape)\n",
    "            y_pred -= np.multiply(self.learning_rate, update)\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = np.array([])\n",
    "        # Make predictions\n",
    "        for tree in self.trees:\n",
    "            update = tree.predict(X)\n",
    "            update = np.multiply(self.learning_rate, update)\n",
    "            y_pred = -update if not y_pred.any() else y_pred - update\n",
    "\n",
    "        y_pred = np.exp(y_pred) / np.expand_dims(np.sum(np.exp(y_pred), axis=1), axis=1)\n",
    "            \n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "model = GradientBoostingClassifier()        \n",
    "model.fit(X,Y)\n",
    "accuracy_score(Y,model.predict(X))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
